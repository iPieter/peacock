{
    "post_title": "RobBERT-2022",
    "abstract": "We update the RobBERT Dutch language model to include new high-frequent tokens present in the latest Dutch OSCAR corpus from 2022. We then pre-train the RobBERT model using this dataset. Our new model is a plug-in replacement for RobBERT and results in a significant performance increase for certain language tasks.", 
    "post_subtitle": "Updating a Dutch Language Model to Account for Evolving Language Use",
    "type": "md",
    "post_tag": "paper",
    "banner": true,
    "theme": "dark",
    "img": "logo.png",
    "author": "Pieter Delobelle",
    "date": "2022-11-15",
    "background": "linear-gradient(45deg, #e3ffe7 0%, #d9e7ff 100%)",
    "pdf": "https://arxiv.org/abs/2211.08192",
    "bibtex": "",
    "absoluteresource": "https://huggingface.co/DTAI-KULeuven/robbert-2022-dutch-base",
    "draft": false,
    "code": "https://github.com/iPieter/robbert",
    "featured": true,
    "project": false,
    "blog": false,
    "related_work": [],
    "video": ""

  }
  
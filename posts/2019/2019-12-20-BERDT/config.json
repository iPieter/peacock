{
  "post_title": "RobBERT",
  "post_subtitle": "A Dutch RoBERTa-based Language Model",
  "abstract": "Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT. Although the multilingual version of BERT performs well on many tasks, recent studies showed that BERT models trained on a single language significantly outperform the multilingual results.<br/>For this reason we present a Dutch model based on RoBERTa, which we call RobBERT. We show that RobBERT improves state of the art results in Dutch-specific language tasks.",
  "type": "md",
  "post_tag": "paper",
  "banner": true,
  "theme": "dark",
  "img": "logo.png",
  "author": "Pieter Delobelle, Thomas Winters and Bettina Berendt",
  "date": "2020-01-20",
  "background": "linear-gradient(00deg, #e3ffe7 0%, #d9e7ff 100%)",
  "background-old": " linear-gradient(to bottom right, rgb(87, 101, 255), rgb(98, 102, 247), rgb(109, 102, 239), rgb(119, 103, 231), rgb(130, 103, 223), rgb(141, 104, 215), rgb(152, 104, 208), rgb(163, 105, 200), rgb(174, 105, 192), rgb(184, 106, 184), rgb(195, 106, 176), rgb(206, 107, 168))",
  "pdf": "https://arxiv.org/abs/2001.06286",
  "bibtex": "",
  "resource": "/robbert#get-started",
  "draft": false,
  "code": "https://github.com/iPieter/robbert",
  "featured": true,
  "project": true,
  "related_work": []

}

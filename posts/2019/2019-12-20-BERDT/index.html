<article>
  <div class="container-fluid" id="app">
    <div class="container">
      <div class="col-lg-8 col-md-10 mx-auto mb-5">
        The advent of neural networks in natural language processing (NLP) has
        significantly improved state-of-the-art results within the field. While
        recurrent neural networks (RNNs) and long short-term memory networks
        (LSTMs) initially dominated the field, recent models started
        incorporating attention mechanisms and then later dropped the recurrent
        part and just kept the attention mechanisms in so-called transformer
        models. This latter type of model caused a new revolution in NLP and led
        to popular language models like
        <a href="https://openai.com/blog/better-language-models/">GPT-2</a> and
        <a href="https://allennlp.org/elmo">ELMo</a>.
        <a href="https://github.com/google-research/bert">BERT</a> improved over
        previous transformer models and recurrent networks by allowing the
        system to learn from input text in a bidirectional way, rather than only
        from left-to-right or the other way around. This model was later
        re-implemented, critically evaluated and improved in the
        <a
          href="https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md"
          >RoBERTa model</a
        >.

        <h2 class="mt-3">The benefits of pre-training</h2>
        These large-scale transformer models provide the advantage of being able
        to solve NLP tasks by having a common, expensive pre-training phase,
        followed by a smaller fine-tuning phase. The pre-training happens in an
        unsupervised way by providing large corpora of text in the desired
        language. The second phase only needs a relatively small annotated data
        set for fine-tuning to outperform previous popular approaches in one of
        a large number of possible language tasks.

        <h2 class="mt-3">The benefits of language-specific LMs</h2>
        While language models are usually trained on English data, some
        multilingual models also exist. These are usually trained on a large
        quantity of text in different languages. For example, Multilingual-BERT
        is trained on a collection of corpora in 104 different languages and
        generalizes language components well across languages. However, models
        trained on data from one specific language usually improve the
        performance of multilingual models for this particular language.
        Training a RoBERTa model on a Dutch dataset thus has a lot of potential
        for increasing performance for many downstream Dutch NLP tasks.
      </div>
    </div>
    <div class="row text-white bg-dark" id="get-started">
      <div class="col-lg-8 col-md-10 mx-auto py-4">
        <h1>Get started with our models</h1>
        <p>
          We release our pretrained models for both
          <a href="https://github.com/huggingface/transformers"
            >Hugging Face's transformers</a
          >
          and
          <a href="https://github.com/pytorch/fairseq">Facebook's Fairseq</a>.
          For some downstream tasks, we also have models in either or both
          formats.
        </p>
        <div class="d-flex justify-content-center">
          <button
            class="btn btn-outline-light mx-3"
            href="#"
            v-bind:class="library=='transformers'? 'active' : ''"
            v-on:click="library='transformers'"
          >
            I'm using ü§ó Transformers
          </button>
          <button
            class="btn btn-outline-light mx-3"
            href="#"
            v-bind:class="library=='fairseq'? 'active' : ''"
            v-on:click="library='fairseq'"
          >
            I'm using Facebook's Fairseq
          </button>
        </div>

        <div v-if="library=='transformers'">
          <p>
            Awesome that you're using ü§ó Transformers. You can import our models
            directly using:
          </p>

          <pre
            class="small pl-4"
            style="margin: 0px; line-height: 125%;"
          ><span style="color: rgb(249, 38, 114);">from</span> <span style="color: rgb(248, 248, 242);">transformers</span> <span style="color: rgb(249, 38, 114);">import</span> <span style="color: rgb(248, 248, 242);">RobertaTokenizer,</span> <span style="color: rgb(248, 248, 242);">RobertaForSequenceClassification</span>
<span style="color: rgb(248, 248, 242);">tokenizer</span> <span style="color: rgb(249, 38, 114);">=</span> <span style="color: rgb(248, 248, 242);">RobertaTokenizer</span><span style="color: rgb(249, 38, 114);">.</span><span style="color: rgb(248, 248, 242);">from_pretrained(</span><span style="color: rgb(230, 219, 116);">"pdelobelle/robBERT-base"</span><span style="color: rgb(248, 248, 242);">)</span>
<span style="color: rgb(248, 248, 242);">model</span> <span style="color: rgb(249, 38, 114);">=</span> <span style="color: rgb(248, 248, 242);">RobertaForSequenceClassification</span><span style="color: rgb(249, 38, 114);">.</span><span style="color: rgb(248, 248, 242);">from_pretrained(</span><span style="color: rgb(230, 219, 116);">"pdelobelle/robBERT-base"</span><span style="color: rgb(248, 248, 242);">)</span>
          </pre>

          <p>
            With the pretrained model being either the base model, that was
            trained on language modeling (LM) or a finetuned model that we
            provide.
          </p>
          <table class="table mt-3">
            <thead>
              <tr>
                <th>Model</th>
                <th>Task</th>
                <th>Accuracy (%)</th>
                <th>F1 (%)</th>
              </tr>
            </thead>
            <tr>
              <td>
                <span class="text-monospace">pdelobelle/robBERT-base</span>
                <button
                  type="button"
                  class="btn btn-outline-light"
                  v-on:click="copyTextToClipboard('pdelobelle/robBERT-base')"
                >
                  <i class="fas fa-copy"></i> Copy
                </button>
              </td>
              <td>LM</td>
              <td>‚Äî</td>
              <td>‚Äî</td>
            </tr>

            <tr>
              <td>
                <span class="text-monospace">pdelobelle/robBERT-dutch-books</span>
                <button
                  type="button"
                  class="btn btn-outline-light"
                  v-on:click="copyTextToClipboard('pdelobelle/robBERT-dutch-books')"
                >
                  <i class="fas fa-copy"></i> Copy
                </button>
              </td>
              <td>DBRD</td>
              <td>94.4%</td>
              <td>94.4%</td>
            </tr>
          </table>
        </div>

        <div v-else-if="library=='fairseq'">
          <p>
            Great that you're using Fairseq! We pre-trained our model with this
            library. You can import this pre-trained version of RobBERT by
            downloading it directly.
          </p>

          <table class="table mt-3">
            <thead>
              <tr>
                <th>Model</th>
                <th>Task</th>
                <th>Accuracy (%)</th>
                <th>F1 (%)</th>
              </tr>
            </thead>
            <tr>
              <td>
                <span class="text-monospace">RobBERT-base</span>
                <a
                  role="button"
                  class="btn btn-outline-light"
                  href="https://github.com/iPieter/BERDT/releases/download/v1.0/RobBERT-base.pt"
                >
                  <i class="fas fa-download"></i> Download (1.4 GB)
                </button>
              </td>
              <td>LM</td>
              <td>‚Äî</td>
              <td>‚Äî</td>
            </tr>
            <tr>
              <td>
                <span class="text-monospace">RobBERT-diedat</span>
                <a
                  role="button"
                  class="btn btn-outline-light"
                  href="https://github.com/iPieter/RobBERT/releases/download/v1.0/RobBERT-diedat.pt"
                >
                  <i class="fas fa-download"></i> Download (1.4 GB)
                </a>
              </td>
              <td><i>Die</i> vs <i>dat</i></td>
              <td>98.4%</td>
              <td>98.1%</td>
            </tr>
          </table>
        </div>
        <h2 class="mt-4 mb-0">A note on copyright</h2>
        <p>We're releasing all our models under <a href="https://github.com/iPieter/RobBERT/blob/master/LICENSE">MIT</a>.</p>
      </div>
    </div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto mt-5">
          <h1>Downstream tasks</h1>
          <h2>Anaphora resolution with <i>die</i> and <i>dat</i></h2>
          <p>
            We evaluated RobBERT's performance on a task that is specific to
            Dutch, namely disambiguating <i>"die"</i> and <i>"dat"</i> (=
            <i>"that"</i> in English). In Dutch, depending on the sentence, both
            terms can be either demonstrative or relative pronouns; in addition
            they can also be used in a subordinating conjunction, i.e. to
            introduce a clause. The use of either of these words depends on the
            gender of the word it refers to. Distinguishing these words is a
            task introduced by
            <a href="https://arxiv.org/abs/2001.02943"
              >Allein <i>et al.</i> (2020)</a
            >, who presented multiple models trained on the Europarl and SoNaR
            corpora. Their results ranged from an accuracy of 75.03% on Europarl
            to 84.56% on SoNaR.
          </p>
          <p>
            For this task, we use the Dutch version of the Europarl corpus,
            which we split in 1.3M utterances for training, 319k for validation,
            and 399k for testing. We then process every sentence by checking if
            it contains <i>"die"</i> and <i>"dat"</i>, and if so, add a training
            example for every occurrence of this word in the sentence, where a
            single occurrence is masked. For the test set for example, this
            resulted in about 289k masked sentences. We then test two different
            approaches for solving this task on this dataset. The first approach
            is making the BERT models use their MLM task and guess which word
            should be filled in this spot, and check if it has more confidence
            in either <i>"die"</i> and <i>"dat"</i> (by checking the first 2,048
            guesses at most, as this seemed sufficiently large). This allows us
            to compare the zero-shot BERT models, i.e. without any fine-tuning
            after pre-training.. The second approach uses the same data, but
            creates two sentences by filling in the mask with both
            <i>"die"</i> and <i>"dat"</i>, appending both with the
            <code>[SEP]</code> token and making the model predict which of the
            two sentences is correct.
          </p>

          <h2 class="mt-4">High-level sentiment analysis</h2>
          <p>
            we compare its performance with other BERT-models and
            state-of-the-art systems in sentiment analysis, to show its
            performance for classification tasks. We replicated the high-level
            sentiment analysis task used to evaluate BERTje to be able to
            compare our methods. This task uses a dataset called Dutch Book
            Reviews Dataset (DBRD), in which book reviews scraped from hebban.nl
            are labeled as positive or negative. Although the dataset contains
            118,516 reviews, only 22,252 of these reviews are actually labeled
            as positive or negative.
          </p>

          <p>
            We trained our model for 2000 iterations with a batch size of 128
            and a warm-up of 500 iterations, reaching a learning rate of 10‚Åª‚Åµ.
            We found that our model performed better when trained on the last
            part of the book reviews than on the first part. This is likely due
            to this part containing concluding remarks summarizing the overall
            sentiment.
          </p>
        </div>
      </div>
    </div>
    <div
      class="row mt-4"
      style="background: linear-gradient(45deg, #e3ffe7cc 0%, #d9e7ffcc 100%);"
    >
      <div class="row">
        <div class="">
          <div class="col-lg-8 col-md-10 mx-auto mt-5">
            <h2>Examples on sentiment analysis</h2>
            <p>
              We've trained and evaluated our model on the Dutch Books Review
              Dataset (DBRD), which is a collection of labeled possitive,
              negative and neutral reviews. This allowed us to do sentiment
              analysis on these reviews. We show the first examples from our
              test set here.
            </p>
          </div>
          <div id="myCarousel" class="carousel slide" data-ride="carousel">
            <!-- Carousel indicators -->
            <ol class="carousel-indicators ">
              <li
                data-target="#myCarousel"
                data-slide-to="0"
                class="active bg-dark"
              ></li>
              <li
                class="bg-dark"
                data-target="#myCarousel"
                data-slide-to="1"
              ></li>
              <li
                class="bg-dark"
                data-target="#myCarousel"
                data-slide-to="2"
              ></li>
              <li
                class="bg-dark"
                data-target="#myCarousel"
                data-slide-to="3"
              ></li>
            </ol>
            <!-- Wrapper for carousel items -->
            <div class="carousel-inner">
              <div class="item carousel-item active">
                <div class="col-md-9 col-center m-auto pb-5">
                  <div class="media">
                    <img
                      src="book_0_front.jpg"
                      class="align-self-start mr-3 col-md-2"
                      alt="Book 0 front"
                    />
                    <div class="media-body">
                      <h5 class="mt-0">Zwanenzang</h5>
                      <h6>By Loes den Hollander</h6>
                      <p>
                        Wat een geweldig boek is dit weer van Loes! Ik kan
                        mezelf inmiddels toch wel een fan noemen hoor. Vrijdag
                        vond ik geweldig maar Zwanenzang doet er zeker niet voor
                        onder. Broeinest heb ik ook gelezen maar vergeleken met
                        deze twee vond ik die toch net iets minder maar ook nog
                        steeds geweldig. Zwanenzang leest als een trein, is
                        nergens saai, blijft spannend tot het laatste hoofdstuk
                        en heeft een geweldige ontknoping! Loes heeft er een fan
                        bij...
                      </p>
                      <img src="book_0.png" alt="Book 2 score" width="300px" />
                    </div>
                  </div>
                </div>
              </div>
              <div class="item carousel-item ">
                <div class="col-md-9 col-center m-auto pb-5">
                  <div class="media">
                    <img
                      src="book_1_front.jpg"
                      class="align-self-start mr-3 col-md-2"
                      alt="Book 1 front"
                    />
                    <div class="media-body">
                      <h5 class="mt-0">Een boek per dag</h5>
                      <h6>By Nina Sankovitch</h6>
                      <p>
                        Ik vond er helemaal niets aan. Het is niet mijn gewoonte
                        maar na 4 hoofdstukken heb ik het opgegeven en het boek
                        weggelegd.
                      </p>
                      <img src="book_1.png" alt="Book 2 score" width="300px" />
                    </div>
                  </div>
                </div>
              </div>
              <div class="item carousel-item ">
                <div class="col-md-9 col-center m-auto pb-5">
                  <div class="media">
                    <img
                      src="book_2_front.jpg"
                      class="align-self-start mr-3 col-md-2"
                      alt="Book 2 front"
                    />
                    <div class="media-body">
                      <h5 class="mt-0">Dorsvloer vol confetti</h5>
                      <h6>By Franca Treur</h6>
                      <p>
                        Het boek kon mij niet boeien, gaf wel herkenning met
                        mijn eigen jeugdleven en opvoeding, maar het is allemaal
                        nogal saai weergegeven.
                      </p>
                      <img src="book_2.png" alt="Book 2 score" width="300px" />
                    </div>
                  </div>
                </div>
              </div>
              <div class="item carousel-item ">
                <div class="col-md-9 col-center m-auto pb-5">
                  <div class="media">
                    <img
                      src="book_3_front.jpg"
                      class="align-self-start mr-3 col-md-2"
                      alt="Book 3 front"
                    />
                    <div class="media-body">
                      <h5 class="mt-0">The Path Keeper</h5>
                      <h6>By N.J. Simmonds</h6>
                      <p>
                        'Can love ever be stronger than fate?' dat staat op de
                        cover van The path keeper en dat vind ik een goede en
                        interessante vraag. The path keeper is een mooi verhaal
                        over leven, lotsbestemming en liefde. N.J Simmonds wist
                        mij op het puntje van mijn stoel te houden met dit
                        verhaal. Er wordt in het verhaal gebruik gemaakt van
                        glimpen uit het verleden. Hele hoofdstukken over vorige
                        levens inclusief bijpassende schrijfstijl. Heel goed
                        gedaan en een goede toevoeging aan het verhaal. Verder
                        vond ik het een origineel verhaal met een interessante
                        kijk op het leven, het lot en religie waardoor je toch
                        anders gaat kijken naar de wereld. Heb ik die persoon om
                        een rede ontmoet? Waarom loopt mijn leven op de manier
                        waarop het loopt? Heb ik iets verkeerds gedaan in een
                        vorig leven? Ik ben gaan meeleven met Ella en vind het
                        erg dat ik nu tot 2018 moet wachten om te wachten hoe
                        het verder gaat. Al heb ik tijdens mijn interview met
                        N.J Simmonds wel een paar teasers gehad over het
                        verhaal. Lees ook het interview met N.J. Simmons
                      </p>
                      <img src="book_3.png" alt="Book 3 score" width="300px" />
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <!-- Carousel controls -->
            <a
              class="carousel-control left carousel-control-prev text-dark"
              href="#myCarousel"
              data-slide="prev"
            >
              <i class="fa fa-angle-left"></i>
            </a>
            <a
              class="carousel-control right carousel-control-next text-dark"
              href="#myCarousel"
              data-slide="next"
            >
              <i class="fa fa-angle-right"></i>
            </a>
          </div>
        </div>
      </div>
    </div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto mt-5">
          <h1 id="name">Finally, a note on the name and the logo</h1>
          <p>
            We named our model RobBERT, or to be more precise: our model named
            itself RobBERT. When we used word masking in a sentence to introduce
            itself, it picked `RobBERT` as the most likely name. In an
            serendipitous way, this also highlighted the link to RoBERTa, so
            that name was perfect!
          </p>

          <p>
            The word <i>rob</i> also means <i>seal</i> in Dutch, hence our logo
            is a seal being dressed up as Bert. Special thanks to Thomas Winters
            for the logo!
          </p>
        </div>
      </div>
    </div>
  </div>
</article>

<script src="https://cdn.jsdelivr.net/npm/vue"></script>
<script>
  var app = new Vue({
    el: "#app",
    data: {
      library: "transformers"
    },
    methods: {
      fallbackCopyTextToClipboard(text) {
        var textArea = document.createElement("textarea");
        textArea.value = text;
        textArea.style.position = "fixed"; //avoid scrolling to bottom
        document.body.appendChild(textArea);
        textArea.focus();
        textArea.select();

        try {
          var successful = document.execCommand("copy");
          var msg = successful ? "successful" : "unsuccessful";
          console.log("Fallback: Copying text command was " + msg);
        } catch (err) {
          console.error("Fallback: Oops, unable to copy", err);
        }

        document.body.removeChild(textArea);
      },
      copyTextToClipboard(text) {
        if (!navigator.clipboard) {
          this.fallbackCopyTextToClipboard(text);
          return;
        }
        navigator.clipboard.writeText(text).then(
          function() {
            console.log("Async: Copying to clipboard was successful!");
          },
          function(err) {
            console.error("Async: Could not copy text: ", err);
          }
        );
      }
    }
  });
</script>

{
  "post_title": "Ethical Adversaries",
  "post_subtitle": "Mitigating Unfairness with Adversarial Machine Learning",
  "abstract": "We offer a new framework that assists in mitigating unfair representations in the dataset used for training. Our framework relies on adversaries to improve fairness. First, it evaluates a model for unfairness w.r.t. protected attributes and ensures that an adversary cannot guess such attributes for a given outcome, by optimizing the model’s parameters for fairness while limiting utility losses. Second, the framework leverages evasion attacks from adversarial machine learning to perform adversarial retraining with new examples unseen by the model. These two steps are iteratively applied until a significant improvement in fairness is obtained.<br/>We evaluated our framework on well-studied datasets in the fairness literature—including COMPAS—where it can surpass other approaches concerning demographic parity, equality of opportunity and also the model’s utility. ",
  "type": "md",
  "banner": true,
  "theme": "dark",
  "img": "architecture.png",
  "author": "Pieter Delobelle, Paul Temple, Gilles Perrouin, Benoît Frénay, Patrick Heymans, and Bettina Berendt",
  "date": "2020-06-04",
  "background": "rgb(248,247,243)",
  "pdf": "https://arxiv.org/abs/2005.06852",
  "bibtex": "",
  "resource": "",
  "draft": true,
  "code": "https://github.com/iPieter/ethical-adversaries"
}